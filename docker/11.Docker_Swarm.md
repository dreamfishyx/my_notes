#### 简介

##### 介绍

1. Docker Swarm 是由 Docker 公司推出的 Docker 的原生集群管理系统，它将一个 Docker 主机池变成了一个单独的虚拟主机，用户只需通过简单的 API 即可实现与 Docker 集群的通信。
2. Docker Swarm 使用 GO 语言开发。从 Docker 1.12.0 版本开始，Docker Swarm 已经内置于 Docker 引擎中，无需再专门的进行安装配置。
3. Docker Swarm 在 Docker 官网的地址为：[官网](https://docs.docker.com/engine/swarm/)。





##### 节点架构

1. 架构图:<br><img src="./assets/swarm-diagram.webp" alt="Swarm mode cluster" style="zoom:67%;" />

2. swarm node:

   1. 从物理上讲，一个 Swarm 是由若干安装了 Docker Engine 的物理机或者虚拟机组成，这些主机上的 Docker Engine 都采用 Swarm模式运行。
   2. 从逻辑上讲，一个 Swarm 由若干节点 node 构成，每个 node 最终会落实在一个物理 Docker 主机上，但一个物理 Docker 主机并不一定就是一个 node。即 <font color=red><swarm node 与 Docker主机并不是一对一的关系</font>。

3. swarm node 共有两种类型：manager 与 worker。

   1. Manager: Manager 节点用于维护 swarm 集群状态、调试 servcie、处理 swarm 集群管理任务。为了防止单点故障问题，一个 Swarm 集群一般都会包含多个 manager。这些 manager 间通过Raft 算法维护着一致性。
   2. Worker: Worker 节点用于在其 Contiainer 中运行 task 任务，即对外提供 service 服务。默认情况下，manager 节点同时也充当着 worker 角色，可以运行 task 任务。

4. 角色转换: manager 节点与 worker 节点角色并不是一成不变的，它们之间是可以相互转换的。

   1. manager 转变为 worker 称为节点降级。
   2. worker 转变为 manager 称为节点升级。

5. 还是要再强调以一下: 每个 node 最终会落实在一个物理 Docker 主机上，但一个物理 Docker 主机并不一定就是一个 node。即 <font color=red><swarm node 与 Docker主机并不是一对一的关系</font>。

   

   

##### 服务架构

1. 架构图:<br><img src="./assets/services-diagram.webp" alt=" HTTP listener service with three replicas" style="zoom:67%;" /><br><img src="./assets/service-lifecycle.webp" alt="Services flow" style="zoom:67%;" />

2. service: 搭建 docker swarm 集群的目的是为了能够在 swarm 集群中运行应用，为用户提供具备更强抗压能力的服务。docker swarm 中的服务 service 就是一个逻辑概念，表示 swarm 集群对外提供的服务。

3. task: 一个 service 最终是通过任务 task 的形式出现在 swarm 的各个节点中，而每个节点中的 task 又都是通过具体的运行着应用进程的容器对外提供的服务。

4. 编排器: 在 swarm manager 中具有一个编排器，用于管理副本 task 任务的创建与停止。例如，当在 swarm manager 中定义一个具有 3 个 task 副本任务的 service 时，编排器首先会创建 3 个 task，为每个 task 分配一个 taskID，并通过分配器为每个 task 分配一个虚拟 IP，即 VIP。然后再将该 task 注册到内置的 DNS 中。当 service 的某 task 不可用时，编排器会在 DNS 中注销该 task。

5. 分发器: 在 swarm manager 中具有一个分发器，用于完成对副本 task 任务的监听、调度等操作。在前面的例子中，当编排器创建了 3 个 task 副本任务后，会调用分发器为每个 task 分配节点。分发器首先会在 swarm 集群的所有节点中找到 3 个 available node 可用节点，每个节点上分配一个 task。而每个 task 就像是一个“插槽”，分发器会在每个“插槽”中放入一个应用容器。每个应用容器其实就是一个具体的 task 实例。一旦应用容器运行起来，分发器就可以监测到其运行状态，即 task 的运行状态。如果容器不可用或被终止，task 也将被终止。此时编排器会立即在内置 DNS 中注销该task，然后编排器会再生成一个新的 task，并在 DNS 中进行注册，然后再调用分发器为之分配一个新的 available node，然后再该节点上再运行应用容器。编排器始终维护着 3 个 task副本任务。分发器除了为 task 分配节点外，还实现了对访问请求的负载均衡。当有客户端来访问swarm 提供的 service 服务时，该请求会被 manager 处理：根据其内置 DNS，实现访问的负载均衡。

6. 在 Swarm 中创建服务时，流程大致如下：

   1. 编排器接收创建服务的请求，确定所需的任务数量。
   2. 编排器创建任务，并为每个任务分配唯一的 ID 和虚拟 IP（VIP）。
   3. 分发器查找可用的工作节点，将任务分配到这些节点。
   4. 一旦任务被分配，分发器监控任务的运行状态，并在需要时进行故障恢复。

   



##### 服务部署模式

1. 示意图:<br><img src="./assets/replicated-vs-global.webp" alt="Global vs replicated services" style="zoom:67%;" />
2. service 以副本任务 task 的形式部署在 swarm 集群节点上。根据 task 数量与节点数量的关系，常见的 service 部署模式有两种：replicated 模式与 global 模式。
   1. replicated 模式: 副本模式，service 的默认部署模式。需要指定 task 的数量。当需要的副本任务 task 数量不等于 swarm 集群的节点数量时，就需要使用 replicated 模式。manager 中的分发器会找到指定 task 个数的 available node 可用节点，然后为这些节点中的每个节点分配一个或若干个 task。
   2. global 模式: 全局模式。分发器会为每个 swarm 集群节点分配一个 task，不能指定 task 的数量。swarm 集群每增加一个节点，编排器就会创建一个 task，并通过分发器分配到新的节点上。
3. 对于含有 5 个节点的集群， replicated 模式下，若是服务存在 6 个 task，那么就会存在一个节点分到两个 task 。但是对于 global 模式下，就只会创建 5 个 task。





#### 搭建集群

> 此处采用ubuntu虚拟机搭建，wsl 导入导出不如虚拟机克隆来得快。我们搭建一个 docker swarm 集群(提前安装好 docker ，并配置加速镜像)，包含 5 个 swarm 节点。这 5 个 swarm 节点的 IP 与暂时的角色分配如下:
>
> | hostname | ip            | role    |
> | -------- | ------------- | ------- |
> | docker1  | 192.168.79.11 | manager |
> | docker2  | 192.168.79.12 | manager |
> | docker3  | 192.168.79.13 | manager |
> | docker4  | 192.168.79.14 | worker  |
> | docker5  | 192.168.79.15 | worker  |





##### 集群初始化

1. 首先我们需要知道:在任意 docker 主机上通过 `docker info` 命令可以查看到当前 docker 引擎 Server 端对于 swarm 的激活状态。

   ```bash
   $ docker info
   Client:
    ...
   Server:
    ...
    Swarm: inactive(未激活)
    ...
   ```

2. 在主机名为 docker1 的主机上运行 docker swarm init 命令，创建并初始化一个 swarm。执行初始化的节点会作为 manager 节点。

   ```bash
   $ docker swarm init 
   Swarm initialized: current node (nn36lc1bs25erh4399ik46le1) is now a manager.
   To add a worker to this swarm, run the following command:
       docker swarm join --token SWMTKN-1-1xfpzxk3cl8d8rxdqixdy5strk1y16j6aa5qn7u87ner6mljq2-cznpgopqwzk3ujjqqd4p0f8eo 192.168.79.11:2377
   To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
   ```

3. 其实通过上述输出结果，可以知道它给出一条添加 worker 节点的方法，就是在其他节点中执行它给出的命令。但是显然我们不可能每次及加入节点都去初始化查看这条命令，我们采用下面的方式查看添加节点的命令。

4. 添加 manager 节点:

   1. 在 docker1 中 运行命令 `docker swarm join-token manager` 获取添加节点命令:

      ```bash
      $ docker swarm join-token manager 
      To add a manager to this swarm, run the following command:
      
          docker swarm join --token SWMTKN-1-1xfpzxk3cl8d8rxdqixdy5strk1y16j6aa5qn7u87ner6mljq2-558rr1ezi2n0377ne5cqmtnri 192.168.79.11:2377
      ```

   2. 在 docker2、docker3 中运行上述命令，从而将它们设置为 manager 节点:

      ```bash
      docker swarm join --token SWMTKN-1-1xfpzxk3cl8d8rxdqixdy5strk1y16j6aa5qn7u87ner6mljq2-558rr1ezi2n0377ne5cqmtnri 192.168.79.11:2377
      
      This node joined a swarm as a manager.
      ```

5. 添加 worker 节点:

   1. 在 docker1 (其实只要是 manager 节点就可)中 运行命令 `docker swarm join-token worker` 获取添加节点命令:

      ```bash
      $ docker swarm join-token worker
      To add a worker to this swarm, run the following command:
      
          docker swarm join --token SWMTKN-1-1xfpzxk3cl8d8rxdqixdy5strk1y16j6aa5qn7u87ner6mljq2-cznpgopqwzk3ujjqqd4p0f8eo 192.168.79.11:2377
      ```

   2. 在 docker4、docker5 中运行上述命令，从而将它们设置为 manager 节点:

      ```bash
      $ docker swarm join --token SWMTKN-1-1xfpzxk3cl8d8rxdqixdy5strk1y16j6aa5qn7u87ner6mljq2-cznpgopqwzk3ujjqqd4p0f8eo 192.168.79.11:2377
      
      This node joined a swarm as a worker.
      ```

6. 在任意的 manager 节点中运行 `docker node ls` 命令可以查看到当前swarm 集群所包含的节点状态数据。<font color=red>需要注意的是: woeker 节点无法运行该命令。</font>

   ```bash
   $ docker node ls
   ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
   nn36lc1bs25erh4399ik46le1     docker1    Ready     Active         Leader           24.0.7
   cyp9sza17yg11uo6v9ftqwyvt     docker2    Ready     Active         Reachable        24.0.7
   hy84w4kr921dls0y0sk182dcp *   docker3    Ready     Active         Reachable        24.0.7
   pzwo3upnqf8my0hqc3j0cajiw     docker4    Ready     Active                          24.0.7
   sbeo4p9wnw3f3h7m8at256tpl     docker5    Ready     Active                          24.0.7
   ```

   > 在 Docker Swarm 中，管理节点（Manager Node）可以分为两种角色：**Leader** 和 **Follower**。
   >
   > 1. Leader 节点是集群中负责决策和管理的主节点。它负责处理所有的管理请求，包括服务创建、更新和删除等操作。
   >    1. 接收来自客户端的请求并进行处理。
   >    2. 维护集群的状态，并将其保存到 Raft 日志中。
   >    3. 负责调度任务到工作节点。
   >    4. 在发生故障时，Leader 节点会重新分配任务，并确保集群的可用性。
   >    5. 当集群中的 Leader 节点出现故障时，其他管理节点会进行选举，选出一个新的 Leader，以确保集群继续正常运作。
   > 2. Follower 节点是集群中的其他管理节点，它们跟随 Leader 节点的指令。
   >    1. 接收 Leader 节点的指令，并将其状态更新与 Leader 节点保持一致。
   >    2. 负责对外提供管理请求的响应，但这些请求会被转发到 Leader 节点处理。
   >    3. 在 Leader 节点失效时，Follower 节点会参与选举，争取成为新的 Leader。
   > 3. 在上述输出中，docker1 是 leader ，而 docker2、docker3 是 follower。而 Reachable 表示当前节点是可以到达的，能访问的。





##### 退出与加入

1. 当一个 worker 节点想从 swarm 集群中退出时，可以通过 `docker swarm leave` 命令。例如将 docker5 退出集群。

2. 此时在 manager 节点中实用 docker node ls 查看发现 docker5 状态变为 Down 。

   ```bash
   $ docker node ls
   ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
   nn36lc1bs25erh4399ik46le1     docker1    Ready     Active         Leader           24.0.7
   cyp9sza17yg11uo6v9ftqwyvt *   docker2    Ready     Active         Reachable        24.0.7
   hy84w4kr921dls0y0sk182dcp     docker3    Ready     Active         Reachable        24.0.7
   pzwo3upnqf8my0hqc3j0cajiw     docker4    Ready     Active                          24.0.7
   sbeo4p9wnw3f3h7m8at256tpl     docker5    Down      Active                          24.0.7
   ```

3. 此时尝试在将 docker5 加入到集群中: 即在 manager 节点上运行 docker swarm join-token worker 命令，生成加入 worker 节点的命令，然后在 docker5 中运行。

4. 此时在 manager 节点中实用 docker node ls 查看发现存在两个 docker5:

   ```bash
   $ docker node ls
   ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
   nn36lc1bs25erh4399ik46le1     docker1    Ready     Active         Leader           24.0.7
   cyp9sza17yg11uo6v9ftqwyvt *   docker2    Ready     Active         Reachable        24.0.7
   hy84w4kr921dls0y0sk182dcp     docker3    Ready     Active         Reachable        24.0.7
   pzwo3upnqf8my0hqc3j0cajiw     docker4    Ready     Active                          24.0.7
   sbeo4p9wnw3f3h7m8at256tpl     docker5    Down      Active                          24.0.7
   xx2ikztjp5ygd4m43fxrhv9jq     docker5    Ready     Active                          24.0.7
   ```

   > 这一幕其实就很好的证明 docker 主机和节点不是一一对应的关系，一个 docker 主机可以对应多个节点。

5. 对于 Down 状态的节点是完全可以将其删除的。通过在manager节点运行 `docker node rm <ID>` 命令完成。

   ```bash
   $ docker node rm sbeo4p9wnw3f3h7m8at256tpl
   sbeo4p9wnw3f3h7m8at256tpl
   ```

   > 实际上这里的 `<id>` 也可以换成主机名，但是当一个主机存在多个节点，此时就会报错，因为无法通过主机名定位到一个唯一的节点。

6. 对于 manager 节点，原则上是不推荐直接退群的，这样会导致 swarm 集群的一致性受到损坏。如果 manager 执意要退群，可在 docker swarm leave 命令后添加-f 或--force 选项进行强制退群。此时需要需要彻底清理和重新初始化 Swarm。

   ```bash
   $ docker swarm leave
   Error response from daemon: You are attempting to leave the swarm on a node that is participating as a manager. The only way to restore a swarm that has lost consensus is to reinitialize it with `--force-new-cluster`. Use `--force` to suppress this message.
   ```

   ```bash
   # 强制退群
   docker swarm leave --force
   
   # 清理和重新初始化
   docker swarm init --force-new-cluster
   ```

   > 虽然不建议这么做，但是有些时候我们不得不这么做，例如后面这种情况。





##### swarm自动锁定

1. 在 manager 集群中，swarm 通过 Raft 日志方式维护了 manager 集群中数据的一致性。即在 manager 集群中每个节点通过 manager 间通信方式维护着自己的 Raft 日志。

2. 但在通信过程中存在有一种风险：Raft 日志攻击者会通过 Raft 日志数据的传递来访问、篡改 **manager** 节点中的配置或数据。<font color=red>为了防止被攻击，swarm 开启了一种集群自动锁定功能，为 **manager** 间的通信启用了 TLS 加密。用于加密和解密的公钥与私钥，全部都维护在各个节点的 Docker 内存中。一旦节点的 Docker 重启，则密钥丢失</font>。

3. swarm 中通过 `autolock `标志来设置集群的自动锁定功能：为 true 则开启自动锁定，为false 则关闭自动锁定:

   ```bash
   fish@docker1:~$ docker info
   Client:
    ...
   
   Server:
    ...
    Swarm: active
     NodeID: nn36lc1bs25erh4399ik46le1
     Is Manager: true
     ClusterID: fxew3qri89196d6jwdkapa4oc
     Managers: 3
     Nodes: 5
     Default Address Pool: 10.0.0.0/8  
     SubnetSize: 24
     Data Path Port: 4789
     Orchestration:
      Task History Retention Limit: 5
     Raft:
      Snapshot Interval: 10000
      Number of Old Snapshots to Retain: 0
      Heartbeat Tick: 1
      Election Tick: 10
     Dispatcher:
      Heartbeat Period: 5 seconds
     CA Configuration:
      Expiry Duration: 3 months
      Force Rotate: 0
     Autolock Managers: false
     Root Rotation In Progress: false
     Node Address: 192.168.79.11			# here
     Manager Addresses:
      192.168.79.11:2377
      192.168.79.12:2377
      192.168.79.13:2377
   ```

   > `docker info`在任意节点中均可查看集群信息，但是在 manager 节点中展示的信息更为丰富。例如的上面的 Autolock 信息，在 worker 节点不输出显示(实际上只显示当前节点信息和 manager 节点 ip等寥寥无几的信息)。

4. 开启集群的自动锁定功能:

   1. 在初始话时使用参数 `--autolock`开启，即`docker swarm init --autolock`。

      > 实际上，初始化时可以配置很多参数，可以使用`docker swarm init --help`查看。

   2. manager 节点通过 `docker swarm update --autolock=true` 命令可以开启当前 swarm 集群的自动锁定功能。

      ```bash
      $ docker swarm update --autolock=true
      Swarm updated.
      To unlock a swarm manager after it restarts, run the `docker swarm unlock`
      command and provide the following key:
      
          SWMKEY-1-I96eIcDKiY87AEAAb/MOXXduTp9HE64RMoxbKF2phvQ
      
      Please remember to store this key in a password manager, since without it you
      will not be able to restart the manager.
      ```

5. 在开启自动锁定后，会给一个秘钥 key ,这个秘钥用于后续解锁使用，我们可以通过命令再次查看秘钥 `docker swarm unlock-key`:

   ```bash
   $ docker swarm unlock-key
   To unlock a swarm manager after it restarts, run the `docker swarm unlock`
   command and provide the following key:
   
       SWMKEY-1-I96eIcDKiY87AEAAb/MOXXduTp9HE64RMoxbKF2phvQ
   
   Please remember to store this key in a password manager, since without it you
   will not be able to restart the manager.
   ```

6. 此时我们关闭 docker3 上的 docker，模拟 docker5 宕机，即执行 `sudo systemctl stop docker`。然后我们再启动 docker:

   ```bash
   sudo systemctl stop docker
   
   sudo systemctl start docker
   ```

7. 此时在 manager 节点中使用 `docker node ls` 发现 docker5 一直处于 Down 状态。

   ```bash
   $ docker node ls
   ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
   nn36lc1bs25erh4399ik46le1     docker1    Ready     Active         Leader           24.0.7
   cyp9sza17yg11uo6v9ftqwyvt *   docker2    Ready     Active         Reachable        24.0.7
   hy84w4kr921dls0y0sk182dcp     docker3    Down      Active         Unreachable      24.0.7
   pzwo3upnqf8my0hqc3j0cajiw     docker4    Ready     Active                          24.0.7
   j462egsc4xhugiykzvesr6x3f     docker5    Ready     Active                          24.0.7
   ```

   > 实际上，没有开启集群的自动锁定时，节点宕机后，重新启动是会自动加入集群的。

8. 并且在 docker3 中运行 `docker info` 查看状态，会发现其 Swarm 模式处于锁定状态: 

   ```bash
   fish@docker3:~$ docker info
   Client:
    ...
   Server:
    ...
    Swarm: locked
    ...
   ```

9. 要想解锁，我们需要在 docker3 中运行 `docker swarm unlock` 命令，解锁 swarm, 该过程需要输入之前的秘钥。解锁完成后，节点就会自动加入集群中。

   ```bash
   $ docker swarm unlock
   Please enter unlock key: 
   ```

   > 需要说明:在 Docker Swarm 中，用于加密和解密的公钥与私钥存储在各个节点的内存中。当节点的 Docker 守护进程重启时，这些密钥会丢失。如果在同一时间所有节点都重启或宕机，密钥将不可用，即使你记住了密钥，节点也无法验证。这种情况下，整个集群将无法恢复，必须重新初始化和搭建。这就是为什么启用自动锁定功能非常重要，以确保在节点重启时能够安全地管理密钥。

